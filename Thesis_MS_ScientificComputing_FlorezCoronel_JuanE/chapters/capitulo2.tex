\chapter{Literature Review}  
%\noindent For tips and guidelines on how to write your Literature Review, visit the GWF clinic section at: https://libguides.uprm.edu/writingclinics, Clinics 2020.

fMRI has emerged as a powerful tool for exploring the inner workings of the human brain, capturing dynamic neural activity, and shedding light on the intricacies of cognition. In tandem, Bayesian statistics has revolutionized data analysis, providing a robust framework for modeling complex systems and making inferences based on probability distributions. However, the intersection of these domains, where fMRI data meets Bayesian statistical methods, remains a fertile ground for exploration \cite{worsley2002general}. Within this multidisciplinary landscape, we embark on a journey that transcends disciplinary boundaries, delving into the realms of fMRI analysis, Bayesian statistics, and their convergence. We venture into the statistical domain of the Limiting Distribution of the Extreme, shedding light on its applications in the context of fMRI data analysis. Alongside, we navigate the world of Gaussian smoothing, a method that has found prominence in preprocessing fMRI data. As our compass, we hold the beacon of research contribution, aiming to synthesize the latest advancements and explore the uncharted territories where these domains coalesce. In this literature review, we navigate through the significant milestones in each of these domains, uncovering their interplay and highlighting our unique contribution to this dynamic field of research.

\section{fMRI: Unlocking the Brain Activity Dynamics}

fMRI stands at the forefront of modern neuroscience, offering a captivating lens through which we can explore the intricate tapestry of human cognition and brain function. To comprehend the marvels of fMRI, we embark on a journey that unfolds with an understanding of its roots in MRI. We'll then traverse the distinct domains that differentiate fMRI from its structural sibling, illuminating the nuanced aspects of this imaging modality. From the granular world of voxels that discretize the brain to the revelation of the BOLD contrast mechanism, we'll delve into the core principles underpinning fMRI. Along this path, we'll uncover how image masking refines the focus of our inquiries, how linear time series analysis unveils the dynamic nature of brain responses, and how essential metrics such as Signal-to-Noise Ratio (SNR), Contrast-to-Noise Ratio (CNR), and the Jaccard Index (JI) empower us to gauge the quality, strength, and reproducibility of our findings. This exploration not only invites us to comprehend the technical intricacies of fMRI but also primes us for a holistic understanding of its applications in unraveling the mysteries of the human mind.

\subsection{MRI}
%- Provide a brief introduction to Magnetic Resonance Imaging (MRI).
%- Explain the fundamental principles of MRI, including the use of strong magnetic fields and radiofrequency pulses.
%- Discuss the production of high-resolution anatomical images in MRI.

MRI is a powerful medical imaging technique that has revolutionized the field of diagnostic medicine \cite{westbrook2018mri}. At its core, MRI relies on the interaction of protons within the human body with strong magnetic fields and radiofrequency pulses. These magnetic fields, often generated by superconducting magnets, align the protons within the body's tissues. Subsequent radiofrequency pulses perturb this alignment, causing the protons to emit radiofrequency signals as they return to their original alignment. By detecting these signals and their variations, MRI scanners create high-resolution anatomical images that provide detailed insights into the internal structures of the body. This non-invasive and versatile imaging modality has become indispensable in clinical diagnosis, research, and medical practice, offering a wealth of information for the assessment of various medical conditions.

\subsection{Differences between MRI and fMRI}
%- Highlight the key differences between traditional MRI and functional MRI (fMRI).
%- Emphasize how fMRI extends the capabilities of MRI by capturing dynamic brain activity.

While conventional MRI primarily focuses on generating static anatomical images of the body's internal structures, fMRI takes a profound leap forward by capturing dynamic brain activity \cite{logothetis2008we}. One of the key differences lies in the data acquisition process: MRI primarily relies on the interaction of protons with magnetic fields to produce detailed anatomical images, whereas fMRI harnesses the BOLD contrast mechanism to indirectly measure neural activity by detecting changes in blood oxygenation levels. This pivotal shift in emphasis allows fMRI to visualize and map brain regions activated during specific cognitive tasks, making it an invaluable tool for cognitive neuroscience and neuropsychology. Unlike traditional MRI, which offers structural insights, fMRI delves into the realm of brain functionality, enabling researchers and clinicians to explore the brain's dynamic responses and connectivity patterns in various states and conditions.

\subsection{Voxels (Volumetric Pixels)}
%- Define voxels and explain their significance in fMRI.
%- Discuss how voxels are used to discretize the brain into 3D units for analysis.

Voxels, short for volumetric pixels, are fundamental building blocks in fMRI data analysis \cite{norman2006beyond}. They represent three-dimensional (3D) units within the brain and play a crucial role in discretizing the brain for analysis. Each voxel corresponds to a tiny, well-defined volume within the brain, and within this volume, fMRI data, particularly BOLD signal measurements, are collected over time. The significance of voxels in fMRI lies in their ability to spatially localize neural activity. By dividing the brain into these discrete units, researchers can precisely pinpoint and analyze the activation patterns across various brain regions. Voxels serve as the basis for constructing functional brain maps, allowing investigators to observe and quantify changes in neural activity associated with different tasks, stimuli, or cognitive processes. This granular spatial resolution is essential for gaining insights into the neural substrates of complex cognitive functions and for advancing our understanding of brain function and organization.

\subsection{BOLD Contrast}
%- Introduce BOLD contrast as a fundamental concept in fMRI.
%- Explain how BOLD contrast is used to indirectly measure brain activity based on changes in blood oxygenation levels.

BOLD contrast stands as a cornerstone in fMRI, offering a unique window into the workings of the human brain \cite{logothetis2004nature}. At its essence, BOLD contrast hinges on the observation that neural activity prompts changes in local blood oxygenation levels. When brain regions become more active, they demand increased oxygen and glucose to sustain their functions. In response, blood flow to these regions surges to meet the demand. Importantly, hemoglobin, the oxygen-carrying molecule in blood, behaves differently when oxygenated and deoxygenated, affecting its magnetic properties. BOLD fMRI capitalizes on these differences by detecting changes in the magnetic properties of blood due to alterations in oxygenation levels. Consequently, it indirectly measures brain activity by monitoring the fluctuations in the BOLD signal, providing researchers with a powerful tool to map and analyze brain regions that are engaged during various cognitive tasks, thus advancing our comprehension of brain function and cognition.

\subsection{Image Masking in fMRI}
%- Describe the concept of image masking in the context of fMRI.
%- Explain how image masking is used to focus the analysis on specific regions of interest (ROIs) in the brain.

In the domain of fMRI, the concept of image masking plays a pivotal role in refining and focusing data analysis. Image masking involves the selective extraction of specific regions of interest (ROIs) from fMRI brain images. These ROIs can correspond to anatomically defined brain structures, functionally significant areas, or areas of interest for a particular study. Image masking is employed to enhance the precision of analyses, as it allows researchers to isolate and concentrate on the neural activity occurring within predefined brain regions. By delineating these ROIs, image masking effectively filters out irrelevant data, reducing noise and enhancing the sensitivity of statistical analyses. This approach proves invaluable in investigating localized brain responses to specific tasks, stimuli, or experimental conditions, contributing to a more detailed understanding of brain function and the neural underpinnings of cognitive processes. One of the most common methods to apply the image masking, as implemented in NiLearn \cite{abraham2014machine}, is based on an heuristic proposed by T.Nichols \cite{luo2003diagnosis}: find the least dense point of the histogram, between a lower cutoff and an upper cutoff of the total image histogram.

\subsection{Time Series Analysis}
%- Discuss the importance of linear time series analysis in fMRI.
%- Explain how linear modeling is applied to fMRI data to extract meaningful information about brain activity.

Linear time series analysis stands as a fundamental technique in fMRI data processing due to its ability to unravel the intricate temporal dynamics of neural activity \cite{friston1994statistical}. fMRI data is acquired as a series of images captured over time, and linear modeling techniques are applied to these time series data to extract meaningful insights about brain activity. Linear models allow researchers to investigate how brain regions respond to various experimental conditions or stimuli. By modeling the relationship between the observed data and experimental variables, linear analysis methods help identify regions that exhibit statistically significant changes in activity, thus revealing the neural substrates underlying specific cognitive processes or tasks. This approach is essential for pinpointing regions of interest, quantifying the magnitude and time course of brain responses, and ultimately advancing our understanding of how the brain functions in response to external stimuli or internal cognitive states.

In mathematical terms \cite{lindquist2008statistical}, let $\bm{y}$ be a vector of the response variable, and $X$ be the design matrix of the study containing the expected BOLD, then:

\begin{equation}
\bm{y} = X \bm{\beta} + \bm{\epsilon}.
\label{eq:linearRegression}
\end{equation}

Where $\bm{\epsilon} \sim N \left(\bm{0},\sigma^2 I\right)$ and $\bm{\beta}$ is the vector of coefficients. Note that by adding this terms we are left with:

\begin{equation}
\bm{y}|\bm{\beta}, \sigma, X \sim N \left(X \bm{\beta},\sigma^2 I\right)
\label{eq:boldDistribution}
\end{equation}

\subsection{fMRI Metrics (SNR, CNR, and JI)}
%- Define and explain the following fMRI metrics:
%  - SNR (Signal-to-Noise Ratio): Discuss its significance in assessing the quality of fMRI data.
%  - CNR (Contrast-to-Noise Ratio): Describe its role in quantifying the strength of activation signals.
%  - Jaccard Index: Explain how it is used to measure the similarity of brain activation patterns across different subjects or conditions.

In \cite{welvaert2013definition}, the authors provide insights into key fMRI metrics, including Signal-to-Noise Ratio (SNR) and Contrast-to-Noise Ratio (CNR), which play pivotal roles in assessing data quality and the strength of activation signals in fMRI. SNR is a critical metric used to evaluate the quality and reliability of fMRI data. It quantifies the ratio of the strength of the signal arising from brain activity to the background noise inherent in the imaging process. Higher SNR values indicate a more robust and detectable signal, which is essential for accurate and meaningful data interpretation. In mathematical terms:

\begin{equation}
\text{SNR} = \frac{\text{median}\{\bm{y}\}}{\sigma_{\bm{\epsilon}}}.
\label{eq:SNRDef}
\end{equation}

CNR is another crucial metric that pertains to fMRI data. CNR assesses the contrast between activated and non-activated brain regions by comparing the difference in signal intensity between them to the noise level \cite{welvaert2013definition}. A higher CNR signifies a stronger and more discernible activation signal relative to background noise, enhancing the ability to detect and interpret neural activity patterns. In mathematical terms:

\begin{equation}
\text{CNR} = \frac{\text{max}\{\bm{y}\}-\text{median}\{\bm{y}\}}.{\sigma_{\bm{\epsilon}}}
\label{eq:CNRDef}
\end{equation}

Additionally, the JI, initially introduced by Paul Jaccard in 1901 \cite{jaccard1901etude}, has found application in the realm of fMRI analysis, as discussed in \cite{maitra2010re}. The JI measures the similarity between two sets by calculating the intersection over the union of their elements. In fMRI, it is used to assess the overlap and consistency of brain activation patterns across different subjects, conditions, or studies. A higher JI indicates greater similarity between activation maps, helping researchers gauge the reproducibility and reliability of fMRI findings, thereby contributing to the robustness of neuroimaging studies.

\begin{equation}
J(\bm{A},\bm{B}) = \frac{|\bm{A} \cap \bm{B}|}{|\bm{A} \cup \bm{B}|}
\label{eq:JIDef}
\end{equation}

\section{Bayesian Statistics: Illuminating Inference with Probabilistic Insight}

In the realm of statistics and data analysis, Bayesian Statistics stands as a beacon of probabilistic reasoning and inference, guiding researchers through the complexities of uncertainty and decision-making. As we embark on our exploration of this methodological approach, we are presented with a distinctive perspective—one that contrasts sharply with the classical methodologies of frequentist statistics. Our journey begins with a general description of Bayesian Statistics, unveiling its core principles and the underlying philosophy that governs its application. Along this path, we'll draw clear distinctions between Bayesian and frequentist statistics, elucidating the fundamental differences in their methodologies and the philosophical underpinnings that set them apart.

With a foundational understanding in place, we delve into the intricacies of prior selection—a pivotal step that shapes Bayesian analysis by incorporating prior beliefs and knowledge into the statistical framework. We'll examine how the application of Bayes' Rule propels Bayesian analysis, enabling the seamless integration of prior information with observed data to construct the posterior distribution, the cornerstone of Bayesian inference. Throughout this exploration, we will illuminate the methodologies and mathematical machinery involved in calculating posterior distributions, demonstrating how Bayesian Statistics empowers researchers to harness the power of probability for robust and insightful data analysis.

As we journey deeper into this realm of probabilistic reasoning, we will unveil the unique contributions and applications of Bayesian Statistics, offering a comprehensive view of its capabilities and the invaluable insights it provides in various fields of research and decision-making.

\subsection{General Description}
%- Provide an overview of Bayesian Statistics as a probabilistic framework for data analysis.
%- Explain the fundamental principles, including the use of probability distributions to model uncertainty.
%- Highlight the Bayesian approach to statistical inference, which is based on updating prior beliefs using observed data.

Bayesian Statistics represents a profound paradigm shift in the realm of data analysis and statistical reasoning \cite{bolstad2016introduction}. At its core, Bayesian Statistics serves as a probabilistic framework that redefines the way we approach uncertainty and make inferences from data. Unlike traditional frequentist statistics, which treats model parameters as fixed and unknown values, Bayesian Statistics treats these parameters as random variables, encapsulating our uncertainty about their values with probability distributions. This foundational shift underscores the Bayesian approach to statistical inference, where prior beliefs about parameters are combined with observed data through Bayes' Rule to construct the posterior distribution. In essence, Bayesian Statistics empowers researchers to continuously update their beliefs in light of new evidence, making it a powerful tool for modeling complex systems, incorporating prior knowledge, and deriving insightful inferences from data in a coherent and intuitive manner.

\subsection{Difference with Frequentist Statistics}
%- Contrast Bayesian Statistics with frequentist (classical) statistics.
%- Discuss the key philosophical and methodological differences between the two approaches, such as the treatment of parameters as random variables in Bayesian Statistics.
%- Explain how Bayesian methods handle uncertainty and incorporate prior information, in contrast to frequentist methods.

One of the central differences lies in their philosophical and methodological foundations. While frequentist statistics relies on a deterministic view of parameters, treating them as fixed, unknown values to be estimated from data, Bayesian Statistics adopts a fundamentally probabilistic perspective \cite{fornacon2022understanding}. In Bayesian inference, parameters are considered as random variables, reflecting our uncertainty about their values before observing data. This treatment allows Bayesian methods to seamlessly handle uncertainty by modeling it explicitly through probability distributions. Moreover, Bayesian Statistics offers a distinctive approach to incorporating prior information into the analysis, where prior beliefs about parameters are integrated with observed data using Bayes' Rule to derive the posterior distribution. In stark contrast, frequentist methods typically do not accommodate prior information in a formal way. These philosophical and methodological disparities underpin the fundamental distinction between Bayesian and frequentist statistics, with Bayesian methods providing a coherent framework for probabilistic inference, robust handling of uncertainty, and seamless integration of prior knowledge.

\subsection{Prior Selection}
%- Explore the concept of prior distributions in Bayesian Statistics.
%- Discuss the significance of prior selection and its impact on posterior inference.
%- Highlight methods and considerations for choosing appropriate prior distributions based on available information and domain expertise.

Prior distributions represent a cornerstone of Bayesian analysis, encapsulating our beliefs and prior knowledge about model parameters before we observe any data \cite{gelman2002prior}. The selection of a prior distribution is a pivotal step in Bayesian modeling, as it exerts a profound influence on the posterior inference. The significance of prior selection lies in its potential to shape the outcome of Bayesian analysis, impacting parameter estimates, uncertainty quantification, and model predictions. When choosing a prior distribution, researchers must strike a delicate balance between incorporating relevant domain expertise and ensuring that the prior does not unduly dominate the posterior. This requires careful consideration of the prior's shape, scale, and informativeness. Various methods, such as non-informative or weakly informative priors, hierarchical modeling, and empirical Bayes techniques, offer strategies for selecting appropriate priors based on the available information and the specific context of the analysis. The judicious choice of prior distributions empowers Bayesian statisticians to incorporate valuable prior knowledge while preserving the capacity of data to update and refine their beliefs, thus yielding more robust and insightful posterior inferences. From \cite{gelman2013bayesian}, we can use a noninformative prior distribution that is uniform on $\left( \bm{\beta}, \log \sigma \right)$:

\begin{equation}
\pi \left( \bm{\beta}, \sigma^2 \right) \propto \frac{1}{\sigma^{2}}
\label{eq:prior}
\end{equation}

\subsection{Bayes' Rule}
%- Explain Bayes' Rule and its central role in Bayesian inference.
%- Show how Bayes' Rule updates prior beliefs to obtain the posterior distribution.
%- Discuss the mathematical formulation and the intuitive interpretation of Bayes' Rule.

Bayes' Rule stands as the keystone of the Bayesian framework, enabling the seamless integration of prior beliefs with observed data to derive the posterior distribution \cite{stone2013bayes}. At its core, Bayes' Rule offers a mathematical expression for updating our initial beliefs (the prior distribution) based on new evidence provided by observed data. It achieves this by considering the likelihood of the data given the parameters of the model. The rule elegantly formulates this process, demonstrating that the posterior distribution is proportional to the product of the prior and the likelihood, suitably normalized. This intuitive interpretation reveals that Bayes' Rule quantifies how our beliefs evolve in light of data, yielding a posterior distribution that encapsulates our updated knowledge about the model parameters. This dynamic interplay between prior beliefs and data-driven information empowers Bayesian inference to not only incorporate existing knowledge but also adapt and refine it as fresh evidence emerges, making it a powerful and flexible tool for statistical analysis and decision-making. From \cite{gelman2013bayesian}, we have the following calculation.

Let's denote the prior distribution of $\bm{\beta}$ by $\pi \left( \bm{\beta} \right)$ and the sampling distribution by $f\left( \bm{y}, \sigma| \bm{\beta} \right)$, then the joint density of $\bm{y}, \sigma$ and $\bm{\beta}$ is given by:

\begin{equation}
f\left(\bm{y},\bm{\beta},\sigma\right) = f\left( \bm{y}, \sigma| \bm{\beta} \right) \pi \left( \bm{\beta} \right)
\label{eq:jointDensity}
\end{equation}

The marginal distribution of $\bm{y}$ is given by:

\begin{equation}
m \left( \bm{y} \right) = \iint f\left( \bm{y}, \sigma | \bm{\beta} \right) \pi \left( \bm{\beta} \right) d \bm{\beta} d \theta
\label{eq:marginalDistribution}
\end{equation}

The posterior distribution of $\bm{\beta}$ is the conditional distribution obtained using the Bayes' Rule:

\begin{equation}
\pi \left( \bm{\beta}|\sigma ,\bm{y} \right) =  \frac{f\left(\bm{y},\bm{\beta},\sigma\right)}{m \left( \bm{y} \right)} = \frac{f\left( \bm{y}| \bm{\beta}, \sigma\right) \pi \left( \bm{\beta},\sigma \right)}{\iint f\left( \bm{y}, \sigma | \bm{\beta} \right) \pi \left( \bm{\beta} \right) d \bm{\beta} d \theta}
\label{eq:posteriorDistribution}
\end{equation}

\subsection{Posterior Distribution Calculation}
%- Describe the process of calculating the posterior distribution in Bayesian analysis.
%- Discuss methods for numerical or analytical computation of posterior distributions, such as Markov Chain Monte Carlo (MCMC) and Maximum A Posteriori (MAP) estimation.
%- Provide insights into the practical application of posterior distributions for making inferences and drawing conclusions.

Bayesian inference hinges on updating prior beliefs about model parameters with observed data to derive the posterior distribution. This process can be computationally demanding, but several methods exist to facilitate the calculation. Numerical techniques such as Markov Chain Monte Carlo (MCMC) and Maximum A Posteriori (MAP) estimation play pivotal roles. MCMC methods, like the Gibbs sampler and Metropolis-Hastings algorithm, generate samples from the posterior distribution, allowing for approximate inference even in complex models. On the other hand, MAP estimation seeks the mode of the posterior distribution, providing a point estimate of the parameters. Practical application of posterior distributions involves making inferences and drawing conclusions about model parameters, predictive distributions, and model fit. Bayesian analysis offers a versatile framework for not only estimating parameter values but also quantifying uncertainty, assessing model adequacy, and generating predictions. By providing a comprehensive account of the uncertainty inherent in statistical models, the posterior distribution equips researchers with a robust tool for informed decision-making and hypothesis testing in various fields of study. As seen in \cite{gelman2013bayesian}, an estimation of the posterior can be deduced given the result obtained in Equation \ref{eq:posteriorDistribution}.

The conditional posterior of $\bm{\beta}$, given $\sigma$ is:

\begin{equation}
\pi \left( \bm{\beta}|\sigma ,\bm{y} \right) \sim N\left( \left( X^TX \right)^{-1}X^T \bm{y}, \left( X^TX \right)^{-1} \sigma^2 \right)
\label{eq:conditionalPosterior}
\end{equation}

Note that $\left( X^TX \right)^{-1}X^T \bm{y}$ is the ordinary least squares solution. From here, we just need to estimate $\sigma^2$. Note, that the marginal posterior of $\sigma^2$ is obtained factoring the joint posterior distribution of $\bm{\beta}$ and $\sigma^2$ as:

\begin{equation}
\pi \left( \sigma^2|\bm{y} \right) = \frac{\pi\left( \bm{\beta},\sigma^2|\bm{y} \right)}{\pi\left( \bm{\beta}|\sigma^2,\bm{y} \right)}
\label{eq:marginalPosteriorSigma2}
\end{equation}

Where:

\begin{equation}
\sigma^2|\bm{y} \sim Inv-\chi^2 \left(n-k,s^2 \right)
\label{eq:sigma2Giveny}
\end{equation}

Where $n$ is the sample size of the data, $k$ is the number of parameters in the data and:

\begin{equation}
s^2=\frac{1}{n-k}\left(\bm{y}-X\left( X^TX \right)^{-1}X^T \bm{y} \right)^T\left(\bm{y}-X\left( X^TX \right)^{-1}X^T \bm{y} \right) 
\label{eq:s2}
\end{equation}

\section{Limiting Distribution of the Extreme: Unveiling the Asymptotic Behavior}

In the study of extreme values, understanding the behavior of extreme events holds paramount importance across various domains, from finance to environmental science. This subsection embarks on a journey into the realm of the Limiting Distribution of the Extreme, a branch of statistics and probability theory that focuses on characterizing the statistical properties of extreme values in large datasets. Our exploration begins with the Truncated Normal Distribution, a probability distribution that plays a pivotal role in modeling bounded extremes. We then navigate the fascinating concept of the Domain of Maximal Attraction, which offers insights into the universality of extreme value distributions across different underlying distributions. Moreover, we delve into the Gumbel Distribution, a cornerstone of extreme value theory renowned for its ability to describe the distribution of maxima. This subsection unfolds as a comprehensive investigation into the statistical foundations of extreme values, unearthing the mathematical principles and distributions that underpin our understanding of rare and extreme events. Through this exploration, we aim to shed light on the behavior of extreme values and their broader implications in various fields of study.

\subsection{Truncated Normal Distribution}
%- Introduce the concept of the Truncated Normal Distribution as a probability distribution used to model extreme values within a specific range.
%- Discuss its properties, including the impact of truncation on distributional characteristics.
%- Explore practical applications and relevance in modeling bounded extremes in various fields.

The Truncated Normal Distribution emerges as a vital probability distribution with wide-ranging applications in modeling extreme values that fall within a specific range \cite{burkardt2014truncated}. This distribution is characterized by the constraint that its values lie within a defined interval, effectively truncating the tails of the standard normal distribution. The truncation of data has a profound impact on distributional properties, causing the Truncated Normal Distribution to deviate from its untruncated counterpart. Notably, the truncation influences statistical moments, such as mean and variance, and alters the distribution's skewness and kurtosis. This makes it a valuable tool in scenarios where extreme values must be bounded, such as in finance, where it can be employed to model stock price returns, or in environmental science, where it finds utility in studying bounded climatic variables. By accommodating constraints on extreme values while maintaining the probabilistic foundations of the normal distribution, the Truncated Normal Distribution offers a flexible framework to analyze and model extreme values across a spectrum of disciplines.

The mean $\mu$ and the variance $\sigma^2$ of the truncated normal distribution can be regarded as a perturbation of the mean $\overline{\mu}$ and variance $\overline{\sigma}^2$ of the parent normal distribution, respectivelly. Its value can be determined by referencing the normal PDF $\phi$ and CDF $\Phi$, as
presented in \cite{johnson1995continuous}. First, define:

\begin{equation}
\alpha = \frac{a-\overline{\mu}}{\overline{\sigma}}; \quad \beta = \frac{b-\overline{\mu}};{\overline{\sigma}}
\label{eq:alpha_beta}
\end{equation}

Then we have:

\begin{equation}
\mu = \overline{\mu} - \overline{\sigma} \cdot \frac{\phi(0,1;\beta)-\phi(0,1;\alpha)}{\Phi(0,1;\beta)-\Phi(0,1;\alpha)}
\label{eq:muDef}
\end{equation}

And:

\begin{equation}
\sigma^2 = \overline{\sigma}^2 \cdot \left( 1 - \frac{\beta \phi(0,1;\beta)- \alpha \phi(0,1;\alpha)}{\Phi(0,1;\beta)-\Phi(0,1;\alpha)} - \left( \frac{\phi(0,1;\beta)-\phi(0,1;\alpha)}{\Phi(0,1;\beta)-\Phi(0,1;\alpha)} \right)^2 \right)
\label{eq:sigma2Def}
\end{equation}

\subsection{Domain of Maximal Attraction}
%- Explain the notion of the Domain of Maximal Attraction (DMA) as a fundamental concept in extreme value theory.
%- Discuss how DMA relates to the asymptotic behavior of extreme value distributions.
%- Highlight the significance of DMA in characterizing the universality of extreme value distributions across different underlying distributions.

In \cite{david2004order}, the concept of the Domain of Maximal Attraction (DMA) takes center stage as a fundamental concept in extreme value theory (EVT). The DMA serves as a crucial cornerstone in understanding the asymptotic behavior of extreme value distributions. At its core, the DMA represents a specific class of distributions that exhibit remarkable convergence properties when dealing with extreme values. It is the set of distributions for which the maxima of independent and identically distributed random variables converge to one of the three extreme value distributions: the Gumbel, Fréchet, or Weibull distribution, depending on the characteristics of the underlying distribution. The significance of DMA lies in its role in characterizing the universality of extreme value distributions. Regardless of the initial distribution of the underlying data, as long as it falls within the DMA, the limiting extreme value distribution will belong to one of the three universality classes. This universality property simplifies the analysis of extreme events and facilitates the generalization of extreme value models across diverse fields, including hydrology, finance, and environmental science, where the behavior of extreme values is of paramount importance. By understanding and leveraging the DMA, researchers gain valuable insights into the commonalities in the extreme value behavior across different data domains and underlying distributions.

From Theorem 10.5.2 of \cite{david2004order} it is deduced that a Truncated Normal distribution ($TN$) is in the domain of maximal attraction of a Gumbel distribution ($G$), See Appendix %\ref{AppendixA}.

Hence, we can say that $\exists a_n>0$ and $b_n$ and a nondegenerate cdf $G$ s.t. $TN^n(a_nx+b_n) \rightarrow G(x)$ at all continuity points of $G$. We can choose:

\begin{equation}
a_n = \left[ n\psi(b_n) \right]^{-1}; \quad b_n = \Psi^{-1}(1-1/n).
\label{eq:an_bn}
\end{equation}

Typically, $\psi$ and $\Psi$ are used as the PDF and CDF of the TN.

\subsection{Gumbel Distribution}
%- Introduce the Gumbel Distribution as a key distribution in extreme value theory, particularly for modeling the distribution of maxima.
%- Describe its mathematical properties, including the location and scale parameters.
%- Explore practical applications of the Gumbel Distribution in analyzing rare and extreme events and its relevance in various fields.

The Gumbel Distribution stands as a cornerstone in the realm of extreme value theory (EVT) \cite{nadarajah2004beta}. This probability distribution holds particular significance in modeling the distribution of maxima, making it an indispensable tool for understanding and quantifying rare and extreme events. Mathematically, the Gumbel Distribution is characterized by two parameters: the location parameter ($\mu$) and the scale parameter ($\sigma$). The location parameter determines the center of the distribution, while the scale parameter governs its spread or variability. Notably, the Gumbel Distribution is a member of the larger class of extreme value distributions and possesses distinctive properties that render it well-suited for capturing the behavior of extreme values. Its tail-heavy characteristics make it particularly relevant in analyzing rare events, such as extreme climatic conditions, financial market crashes, or structural engineering failures. The Gumbel Distribution finds application across various fields, including hydrology for modeling flood magnitudes, finance for assessing the risk of extreme financial losses, and environmental science for studying extreme temperature or precipitation patterns. Its ability to model the tails of distributions effectively makes it an essential tool for characterizing and mitigating the impact of extreme events in diverse domains.

\section{Gaussian Smoothing: Unveiling the Power of Spatial Filtering}

The world of image and signal processing has been greatly enriched by the concept of Gaussian Smoothing, a technique that has found widespread application in diverse domains, from medical imaging to computer vision. In this subsection, we embark on a journey into the realm of Gaussian Smoothing, unraveling its method description and delving into the profound implications of its implementation. Our exploration begins by elucidating the core principles of Gaussian Smoothing as a spatial filtering technique. We'll examine how it operates to reduce noise and enhance features in digital images and signals. Moreover, we'll scrutinize the mathematical underpinnings of the Gaussian kernel and its influence on the smoothing process. Beyond the technical intricacies, we'll delve into the practical implications of Gaussian Smoothing across a spectrum of applications. We'll explore how this method can be harnessed to improve image quality, aid in feature extraction, and enable further analyses in fields as diverse as medical imaging, computer graphics, and neuroscience. By navigating through the methodological intricacies and practical consequences of Gaussian Smoothing, we aim to shed light on its versatile and transformative capabilities in image and signal processing.

\subsection{Method Description}
%- Provide a comprehensive description of Gaussian Smoothing as a spatial filtering technique.
%- Explain the underlying principles and mathematics of how Gaussian Smoothing operates.
%- Discuss the role of the Gaussian kernel in the smoothing process and how it influences the degree of smoothing.

Gaussian Smoothing, stands as a foundational spatial filtering technique widely employed in image and signal processing \cite{garg2016quality}. At its core, Gaussian Smoothing aims to enhance images by reducing noise and preserving essential features. The method operates by applying a Gaussian kernel, characterized by its bell-shaped curve, to each pixel in an image or element in a signal. This kernel serves as a weighted averaging filter, where the central pixel or element receives the highest weight, and the surrounding pixels or elements contribute with decreasing weights as their distance from the center increases. The mathematical basis of Gaussian Smoothing lies in convolution, where the kernel is convolved with the input data, effectively blurring the image or signal. The degree of smoothing is modulated by the standard deviation of the Gaussian kernel; a larger standard deviation results in more significant smoothing. Gaussian Smoothing is fundamentally characterized by its ability to reduce high-frequency noise while preserving the essential structures and features, making it a versatile and indispensable tool in various image processing applications, including edge detection, feature extraction, and noise reduction. Its elegant mathematical foundation and adaptability render it a cornerstone in the toolbox of image and signal processing practitioners. In two dimensions the Gaussian kernel is defined as follows:

\begin{equation}
GK(x,y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2+y^2}{2\sigma^2}}
\label{eq:gaussianKernel}
\end{equation}

\subsection{Implications of Implementation}
%- Explore the practical implications and applications of Gaussian Smoothing.
%- Discuss the implementation using python's scipy of Gaussian Smoothing affects image quality, noise reduction, and feature enhancement.
%- Highlight the relevance of Gaussian Smoothing in various fields, such as medical imaging, computer vision, and neuroscience.

Gaussian Smoothing, implemented using Python's SciPy library \cite{nmeth2020scipy}, holds profound practical implications and applications across a multitude of fields. In image processing, Gaussian Smoothing plays a pivotal role in enhancing image quality by effectively reducing noise, particularly in low-contrast regions. By applying a Gaussian filter, high-frequency noise is attenuated, resulting in a smoother image while preserving critical structures and edges. This technique is instrumental in improving the visual aesthetics of images and facilitating subsequent analysis tasks, such as object recognition and segmentation, in computer vision applications. Moreover, Gaussian Smoothing finds relevance in the domain of medical imaging, where it aids in enhancing the clarity of medical scans and the detection of anomalies. In neuroscience, this method contributes to the preprocessing of functional MRI data by reducing noise and enhancing the visibility of brain activation patterns. Overall, Gaussian Smoothing stands as a versatile tool with broad applications in diverse fields, demonstrating its capacity to enhance data quality, reduce noise, and accentuate important features in various image and signal processing scenarios.

\section{Research Contribution: Advancing Analytical Techniques}

Within the ever-evolving landscape of data analysis on fMIR studies, this subsection delves into the innovative contributions that have extended the horizons of analytical techniques. Here, we explore two distinct facets of research contribution: the Bayesian Model as Compared to the Frequentist Approach and the Thresholding and Smoothing Algorithm.

Firstly, we embark on a journey into the Bayesian Model, a paradigm that redefines statistical inference by embracing the inherent uncertainty within data. Contrasted with the classical frequentist approach, the Bayesian model introduces a fresh perspective, treating model parameters as random variables and integrating prior beliefs with observed data through Bayes' Rule. This paradigm shift holds profound implications for fields ranging from machine learning to scientific research, elevating our capacity to model complex systems and make informed decisions.

In tandem, we explore the Thresholding and Smoothing Algorithm, a versatile tool that leverages spatial filtering to extract valuable features and reduce noise from data. This approach, particularly influential in image and signal processing, encompasses techniques such as Gaussian smoothing and thresholding, which serve to refine the quality and information content of data. The application of this algorithm extends to diverse domains, including medical imaging, computer vision, and remote sensing, ushering in new horizons for data-driven insights.

Through a detailed exploration of these research contributions, we aim to illuminate the transformative impact they have on data analysis on fMRI studies and their applicability in a multitude of fields.

\subsection{Bayesian Model as Compared to Frequentist Approach}
%- Explore the innovative contributions of the Bayesian Model in contrast to the traditional frequentist approach.
%- Discuss the fundamental differences in their methodologies, including the treatment of parameters as random variables and the integration of prior beliefs with observed data through Bayes' Rule.
%- Highlight the impact of the Bayesian paradigm shift on data analysis, inference, and decision-making across various fields.

The Bayesian Model, as exemplified in contrast to the traditional frequentist approach \cite{almodovar2019fast}, marks a significant departure from established methodologies. It heralds a paradigm shift by actively embracing the intrinsic uncertainties inherent in both data and model parameters. In stark contrast to the frequentist methodology, which treats parameters as fixed but unknown values, estimated solely from observed data, the Bayesian Model treats these parameters as dynamic entities, represented as random variables encapsulating our prior beliefs and uncertainties through probability distributions. This innovative approach is firmly rooted in Bayes' Rule, which seamlessly integrates prior beliefs with observed data to derive the posterior distribution, thereby offering a coherent and intuitive framework for statistical inference. In this context, while frequentist approaches yield statistical parametric mappings (SPM), the Bayesian model yields probabilistic mappings, signifying a profound shift in the interpretation and representation of analytical results.

\subsection{Thresholding and Smoothing Algorithm}
%- Examine the transformative contributions of Thresholding and Smoothing Algorithms in data analysis.
%- Discuss the core principles of these algorithms, including techniques such as Gaussian smoothing and thresholding, and their application in enhancing data quality and feature extraction.
%- Explore the wide-ranging practical implications of these algorithms in fields such as medical imaging, computer vision, and remote sensing.

Thresholding and Smoothing Algorithm can make a transformative contribution to the realm of activation detection in fMRI. This approach finds application in image segmentation, data compression, and signal denoising. Moreover, the works \cite{deneux2006using}, \cite{lai1999novel}, and  \cite{orchard2003simultaneous} demonstrates different approaches to the problem, such as SPMs, local principal component analysis (LPCA) and least-squares solutions.


